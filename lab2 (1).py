# -*- coding: utf-8 -*-
"""Lab2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w230nLuAAAUs5FYOFjR2fJSXv8cNtRlP
"""

!pip install nltk
import pandas as pd
import json
from google.colab import drive
import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt_tab')
import re

# Mount Google Drive
drive.mount('/content/drive')

# Path to the file in Google Drive
file_path = '/content/drive/My Drive/a2/NoisyText.txt'
file_path_2 = '/content/drive/My Drive/a2/vdoLinks.csv'
file_path_3 = '/content/drive/My Drive/a2/en.txt'
file_path_4 = '/content/drive/My Drive/a2/abbreviations.json'

# Read the file
# Read the file while ignoring problematic characters
with open(file_path, 'r', encoding='utf-8', errors='replace') as file:
    text = file.read()
with open(file_path_3, 'r') as sw_file:
    swear_words = sw_file.read().splitlines()
with open(file_path_4, 'r') as abbr_file:
    abbreviations = json.load(abbr_file)

# Load the CSV while replacing problematic characters
movie_list = pd.read_csv(file_path_2)

abbreviations

movie_list

def parse_data(data):
    sections = data.split('NewMovieDrPQRd')
    parsed = []
    comments_pattern = r"^[^<'].*$"  # Lines not starting with '<' or '\''
    for section in sections:
        lines = section.strip().split('\n')
        if len(lines) > 0:
            movie_id = lines[0].strip()
            comments = [line for line in lines[1:-1] if re.match(comments_pattern, line)]
            if not comments or comments == movie_id:
                comments = ['No comments were found']
            comments = '\n'.join(comments).strip()
            parsed.append({'youtubeId': movie_id, 'Comments': comments})
    return pd.DataFrame(parsed)

def clean_data(movies_df):
    movies_df = movies_df.dropna(subset=['youtubeId'])

    movies_df['Comments'] = movies_df.apply(lambda row: 'No comments were found' if row['youtubeId'] == row['Comments'] else row['Comments'], axis=1)

    movies_df['Comments'] = movies_df.apply(lambda row: row['Comments'].replace(row['youtubeId'], ''), axis=1)

    #Replacing swear words
    swear_pattern = re.compile(r"\b(" + "|".join(map(re.escape, swear_words)) + r")\b", flags=re.IGNORECASE)
    # Replacing Slangs
    for key in abbreviations.keys():
            movies_df["Comments"] = movies_df["Comments"].str.replace(key, "**", regex=False)

    movies_df["Comments"] = movies_df["Comments"].str.replace(swear_pattern, "**", regex=True)

    return movies_df

# Merge to get movie names
def merge_data(movies_df):
    movies_df = movies_df.merge(movie_list, how='left', left_on='youtubeId', right_on='youtubeId')

    movies_df['title'] = movies_df['title'].str.replace(r'\s*\(\d{4}\)$', '', regex=True)

    return movies_df

def create_cleaned_file(movies_df):
# Step 3: Create the output text file
    cleaned_df = clean_data(movies_df)
    merged_df = merge_data(cleaned_df)
    with open('movie_comments.txt', 'w', encoding='utf-8') as file:
        for _, row in merged_df.iterrows():
            if pd.notnull(row['title']):
                file.write(f"Movie Name: {row['title']}\n")
                file.write(f"The Comments are:\n{row['Comments']}\n\n")

    print("The file 'movie_comments.txt' has been created successfully.")
def count_words(text):
    words = word_tokenize(text)
    return len(words)

if __name__ == '__main__':
    words_before = count_words(text)
    movies_df = parse_data(text)
    create_cleaned_file(movies_df)
    with open('movie_comments.txt', 'r', encoding='utf-8') as file:
      cleaned_text = file.read()
    words_after = count_words(cleaned_text)
    # Calculate the number of letters (characters excluding spaces)
    letters_before = len(text.replace(" ", ""))  # Exclude spaces
    letters_after = len(cleaned_text.replace(" ", ""))    # Exclude spaces

    # Calculate the ratio (LETTERSafter) / (LETTERSbefore)
    ratio = letters_after / letters_before if letters_before != 0 else 0
    # Calculate the ratio (LETTERSafter) / (LETTERSbefore)
    ratio = letters_after / letters_before if letters_before != 0 else 0

    # Output the results
    print(f"Word count before cleaning: {words_before}")
    print(f"Word count after cleaning: {words_after}")
    print(f"Letter ratio (LETTERSafter) / (LETTERSbefore): {ratio:.2f}")

print(cleaned_text)